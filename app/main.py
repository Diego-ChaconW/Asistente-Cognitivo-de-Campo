"""
Aplicaci√≥n principal de Streamlit para el chat RAG con manuales biom√©dicos.
"""
import streamlit as st
from app.config import load_config
from app.services.rag_pipeline import RAGPipeline

# Configurar p√°gina
st.set_page_config(
    page_title="Chat con Manuales Biom√©dicos",
    page_icon="üè•",
    layout="wide"
)

# Inicializar configuraci√≥n
try:
    config = load_config()
except ValueError as e:
    st.error(f"Error de configuraci√≥n: {str(e)}")
    st.stop()

# Inicializar pipeline RAG (una sola vez, usando cache)
@st.cache_resource
def get_rag_pipeline():
    """Inicializa y cachea el pipeline RAG."""
    return RAGPipeline(
        search_config=config.azure_search,
        openai_config=config.azure_openai
    )

rag_pipeline = get_rag_pipeline()

# Inicializar historial de chat en session_state
if "messages" not in st.session_state:
    st.session_state.messages = []

# T√≠tulo y descripci√≥n
st.title("üè• Chat con Manuales Biom√©dicos")
st.markdown("""
**Aplicaci√≥n RAG (Retrieval Augmented Generation)** para consultar manuales t√©cnicos y de usuario 
de dispositivos biom√©dicos usando Azure AI Search y Azure OpenAI.

Esta herramienta est√° dise√±ada para ayudar a **field engineers** a encontrar informaci√≥n t√©cnica 
durante el mantenimiento de equipos.
""")

# Sidebar con par√°metros y configuraci√≥n
with st.sidebar:
    st.header("‚öôÔ∏è Configuraci√≥n")
    
    # Par√°metros ajustables
    top_k = st.slider(
        "N√∫mero de documentos a recuperar (top_k)",
        min_value=1,
        max_value=10,
        value=3,
        help="Cantidad de fragmentos de manuales que se usar√°n como contexto. Valores m√°s bajos usan menos tokens y reducen el riesgo de l√≠mites de tasa."
    )
    
    temperature = st.slider(
        "Temperatura del modelo",
        min_value=0.0,
        max_value=1.0,
        value=1.0,
        step=0.1,
        help="Valores m√°s bajos dan respuestas m√°s deterministas. Nota: Algunos modelos solo soportan el valor por defecto (1.0)"
    )
    
    st.divider()
    
    st.info("üí° **Optimizaci√≥n de tokens**: El sistema limita autom√°ticamente el tama√±o del contexto para evitar l√≠mites de tasa. Los chunks muy largos se truncar√°n si es necesario.")
    
    st.divider()
    
    st.subheader("üìñ Instrucciones de uso")
    st.markdown("""
    **Ejemplos de preguntas:**
    - "¬øC√≥mo calibro el sensor de ox√≠geno del modelo X?"
    - "¬øCu√°l es el procedimiento de mantenimiento preventivo?"
    - "¬øQu√© c√≥digo de error significa E-123?"
    - "¬øC√≥mo cambio el filtro del dispositivo Y?"
    
    **Consejos:**
    - S√© espec√≠fico con modelos y n√∫meros de parte
    - Usa t√©rminos t√©cnicos cuando los conozcas
    - Si no encuentras respuesta, reformula la pregunta
    """)
    
    # Bot√≥n para limpiar conversaci√≥n
    if st.button("üóëÔ∏è Limpiar conversaci√≥n", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# Cuerpo principal: historial de chat
st.subheader("üí¨ Conversaci√≥n")

# Mostrar historial de mensajes
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # Mostrar fuentes si existen (solo para mensajes del asistente)
        if message["role"] == "assistant" and "sources" in message:
            sources = message["sources"]
            if sources:
                st.markdown("---")
                st.markdown("**üìö Fuentes utilizadas:**")
                for i, source in enumerate(sources, 1):
                    source_name = source.get("source", "Unknown")
                    score = source.get("score", 0.0)
                    
                    source_text = f"{i}. {source_name}"
                    if score > 0:
                        source_text += f" - Relevancia: {score:.2f}"
                    
                    st.caption(source_text)

# Campo de entrada para nueva pregunta
if prompt := st.chat_input("Escribe tu pregunta sobre los manuales biom√©dicos..."):
    # A√±adir mensaje del usuario al historial
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Mostrar mensaje del usuario
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Generar respuesta usando RAG
    with st.chat_message("assistant"):
        with st.spinner("Buscando en los manuales y generando respuesta..."):
            try:
                # Llamar al pipeline RAG
                result = rag_pipeline.rag_answer(
                    user_question=prompt,
                    top_k=top_k,
                    temperature=temperature
                )
                
                answer = result["answer"]
                sources = result["sources"]
                
                # Mostrar respuesta
                st.markdown(answer)
                
                # Mostrar fuentes
                if sources:
                    st.markdown("---")
                    st.markdown("**üìö Fuentes utilizadas:**")
                    for i, source in enumerate(sources, 1):
                        source_name = source.get("source", "Unknown")
                        page = source.get("pageNumber")
                        score = source.get("score", 0.0)
                        
                        source_text = f"{i}. {source_name}"
                        if page is not None:
                            source_text += f" (p√°g. {page})"
                        if score > 0:
                            source_text += f" - Relevancia: {score:.2f}"
                        
                        st.caption(source_text)
                
                # Guardar respuesta en el historial
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": answer,
                    "sources": sources
                })
                
            except Exception as e:
                error_msg = f"‚ùå Error al procesar la pregunta: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": error_msg
                })

# Footer
st.divider()
st.caption("üí° Esta aplicaci√≥n usa Azure AI Search para b√∫squeda sem√°ntica y Azure OpenAI para generaci√≥n de respuestas.")

